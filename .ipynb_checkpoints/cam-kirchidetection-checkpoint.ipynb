{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import datetime\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadDataset(Dataset):\n",
    "    def __init__(self,typ):\n",
    "        self.samples = []\n",
    "        for label,i in enumerate(os.listdir(\"data/deepdata/\"+typ)):\n",
    "            for j in os.listdir(\"data/deepdata/\"+typ+\"/\"+i):\n",
    "                im_bgr = cv2.imread(\"data/deepdata/\"+typ+\"/\"+i+\"/\"+j)\n",
    "                im_rgb = im_bgr[:, :, ::-1]\n",
    "                self.samples.append((im_rgb,label,\"data/deepdata/\"+typ+\"/\"+i+\"/\"+j))\n",
    "#                 print(im_rgb.shape)\n",
    "            \n",
    "        self.data_transforms = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(25,),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.samples[index][0]\n",
    "        image = cv2.resize(image,(170,25))\n",
    "        image = self.data_transforms(image)\n",
    "        \n",
    "        return (image,self.samples[index][1],self.samples[index][2])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "threadtrainDataset = ThreadDataset(\"train\")\n",
    "threadvalDataset = ThreadDataset(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\":torch.utils.data.DataLoader(threadtrainDataset, batch_size=4,\n",
    "                                             shuffle=True, num_workers=4),\n",
    "               \"val\":torch.utils.data.DataLoader(threadvalDataset, batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              }\n",
    "\n",
    "dataset_sizes = {\"train\":len(threadtrainDataset), \"val\":len(threadvalDataset) }\n",
    "\n",
    "# print(dataloaders)\n",
    "class_names = [\"c\",\"t\"]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes,path = next(iter(dataloaders['train']))\n",
    "print(classes)\n",
    "# # Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for (inputs, labels,paths) in dataloaders[phase]:\n",
    "#                 print(\"length of data\",inputs.shape)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels,paths) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            print(datetime.datetime.now())\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            print(datetime.datetime.now())\n",
    "            print(preds)\n",
    "#             if 1 not in preds:\n",
    "#                 continue\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(25,),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "#                 transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft.load_state_dict(torch.load(\"./kirchi_model.pth\"))\n",
    "\n",
    "fc = model_ft.fc.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0312, -0.0510, -0.0089,  ..., -0.0499,  0.0312,  0.0417],\n",
       "                      [ 0.0022,  0.0381,  0.0132,  ...,  0.0100, -0.0123,  0.0228]])),\n",
       "             ('bias', tensor([-0.0023, -0.0301]))])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc[\"weight\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalConv = nn.Conv2d(512, 2, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalConv.load_state_dict({\"weight\":fc[\"weight\"].view(2, 512, 1, 1),\n",
    "                           \"bias\":fc[\"bias\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fl = nn.Sequential(*list(model_ft.children())[:-2]+[finalConv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.to(device)\n",
    "model_fl.to(device)\n",
    "model_ft.eval()\n",
    "model_fl.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 25, 170])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (inputs, labels,paths) in dataloaders[\"val\"]:\n",
    "#print(\"length of data\",inputs.shape)\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output torch.Size([1, 2])\n",
      "cam torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model_ft(inputs[0,:,:,:].unsqueeze(0)).sigmoid()\n",
    "    cam = model_fl(inputs[0,:,:,:].unsqueeze(0))\n",
    "print(\"output\",out.size())\n",
    "print(\"cam\",cam[0,1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "29.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "61.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "18.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "98.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "62.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "54.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "25.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "13.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "64.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "91.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "80.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "37.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "16.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "49.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "47.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "28.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "104.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "58.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "63.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "88.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "46.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "10.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "97.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "4.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "102.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "9.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "67.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "5.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "75.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "15.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "53.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "94.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "51.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "99.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "82.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "7.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "70.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "39.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "12.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "40.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "14.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "50.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "83.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "2.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "86.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "20.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "87.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "33.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "23.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "19.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "65.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "24.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "26.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "31.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "66.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "32.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "95.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "38.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "17.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "36.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "22.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "1\n",
      "(40, 175)\n",
      "------------\n",
      "6.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "81.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n",
      "106.jpg\n",
      "torch.Size([3, 25, 170])\n",
      "torch.Size([1, 3, 25, 170])\n",
      "0\n",
      "(40, 175)\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir(\"/home/cvr/Pictures/slvision1/kir-raj\"):\n",
    "    img = cv2.imread(\"/home/cvr/Pictures/slvision1/kir-raj/\"+i)\n",
    "    disp_img = img.copy()\n",
    "    img = cv2.resize(img,(170,25))\n",
    "    print(i)\n",
    "\n",
    "\n",
    "    img = data_transforms(img)\n",
    "#     img = torch.from_numpy(img).permute(2,1,0)\n",
    "    print(img.size())\n",
    "    img = img.to(device)\n",
    "#     print(img.shape)\n",
    "    print(img.unsqueeze(0).shape)\n",
    "    output  = model_ft(img.unsqueeze(0))\n",
    "    _, preds = torch.max(output, 1)\n",
    "    print(preds[0].numpy())\n",
    "    cam = model_fl(img.unsqueeze(0))\n",
    "    cam = cam.detach().numpy()\n",
    "#     print(disp_img.shape)\n",
    "    cam = cv2.resize(np.array(cam[0,preds[0].numpy()]), dsize=(disp_img.shape[1],disp_img.shape[0]))\n",
    "    print(cam.shape)\n",
    "    print(\"------------\")\n",
    "    \n",
    "    camFur = (cam*(256/np.max(cam)))\n",
    "#     heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
    "    cv2.imwrite(\"/home/cvr/Pictures/slvision1/res/\"+i[:-4]+\"_\"+str(preds[0].numpy())+\".jpg\",camFur)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f45bb049a90>"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(cam.dtype)\n",
    "\n",
    "cam = (cam-np.mean(cam))/np.std(cam)\n",
    "camFur = cam*(256/np.max(cam))\n",
    "cam = cam.astype('uint8')\n",
    "# print(disp_img[:,:,0].shape)\n",
    "heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
    "# result = disp_img.copy()\n",
    "# result[:,:,0] = cam * 1 + disp_img[:,:,0] * 0.5\n",
    "# result[:,:,1] = cam * 1 + disp_img[:,:,1] * 0.5\n",
    "# result[:,:,2] = cam * 1+ disp_img[:,:,2] * 0.5\n",
    "\n",
    "\n",
    "\n",
    "# print(result)\n",
    "plt.imshow(heatmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f45bb4805d0>"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(camFur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "camFur = cam*(256/np.max(cam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.99706712189537"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256/np.max(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68311787"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in models.resnet18(pretrained=True).parameters():\n",
    "#     print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "# for param in model_ft.parameters():\n",
    "#     param.requires_grad = False\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4776 Acc: 0.8009\n",
      "val Loss: 0.8924 Acc: 0.4805\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.4290 Acc: 0.8275\n",
      "val Loss: 0.2618 Acc: 0.9027\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.3571 Acc: 0.8839\n",
      "val Loss: 0.5369 Acc: 0.7802\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.2936 Acc: 0.9052\n",
      "val Loss: 0.2360 Acc: 0.9047\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.2346 Acc: 0.9180\n",
      "val Loss: 0.5404 Acc: 0.7607\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.2380 Acc: 0.9191\n",
      "val Loss: 0.1645 Acc: 0.9261\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.1314 Acc: 0.9553\n",
      "val Loss: 0.0771 Acc: 0.9786\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.0966 Acc: 0.9723\n",
      "val Loss: 0.0545 Acc: 0.9922\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.0725 Acc: 0.9808\n",
      "val Loss: 0.0488 Acc: 0.9825\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.0629 Acc: 0.9830\n",
      "val Loss: 0.0586 Acc: 0.9786\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.0517 Acc: 0.9840\n",
      "val Loss: 0.0358 Acc: 0.9883\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.0387 Acc: 0.9894\n",
      "val Loss: 0.0305 Acc: 0.9922\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.0480 Acc: 0.9904\n",
      "val Loss: 0.0417 Acc: 0.9883\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.0301 Acc: 0.9936\n",
      "val Loss: 0.0387 Acc: 0.9883\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.0291 Acc: 0.9947\n",
      "val Loss: 0.0383 Acc: 0.9883\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.0255 Acc: 0.9936\n",
      "val Loss: 0.0403 Acc: 0.9883\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.0231 Acc: 0.9968\n",
      "val Loss: 0.0363 Acc: 0.9903\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.0249 Acc: 0.9936\n",
      "val Loss: 0.0347 Acc: 0.9903\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.0259 Acc: 0.9936\n",
      "val Loss: 0.0336 Acc: 0.9903\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.0237 Acc: 0.9947\n",
      "val Loss: 0.0292 Acc: 0.9922\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.0242 Acc: 0.9936\n",
      "val Loss: 0.0389 Acc: 0.9883\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.0205 Acc: 0.9957\n",
      "val Loss: 0.0373 Acc: 0.9864\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.0214 Acc: 0.9936\n",
      "val Loss: 0.0358 Acc: 0.9903\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.0176 Acc: 0.9979\n",
      "val Loss: 0.0327 Acc: 0.9922\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.0257 Acc: 0.9947\n",
      "val Loss: 0.0286 Acc: 0.9922\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.0225 Acc: 0.9947\n",
      "val Loss: 0.0337 Acc: 0.9922\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.0183 Acc: 0.9979\n",
      "val Loss: 0.0373 Acc: 0.9883\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.0204 Acc: 0.9947\n",
      "val Loss: 0.0418 Acc: 0.9903\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.0207 Acc: 0.9947\n",
      "val Loss: 0.0362 Acc: 0.9922\n",
      "\n",
      "Training complete in 3m 56s\n",
      "Best val Acc: 0.992218\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobilenet_model = models.mobilenet_v2(pretrained=True)\n",
    "# # for param in mobilenet_model.parameters():\n",
    "# #     param.requires_grad = False\n",
    "# # print(mobilenet_model)\n",
    "# num_ftrs = mobilenet_model.classifier[1].in_features\n",
    "# # Here the size of each output sample is set to 2.\n",
    "# # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "# mobilenet_model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# mobilenet_model = mobilenet_model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Observe that all parameters are being optimized\n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobilenet_model = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
